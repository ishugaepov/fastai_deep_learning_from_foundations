{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.nb_01 import *\n",
    "\n",
    "\n",
    "x, y = get_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]), torch.Size([60000]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, m, s): return (x - m) / s\n",
    "\n",
    "\n",
    "x_train, x_test = x[:50000], x[50000:]\n",
    "y_train, y_test = y[:50000], y[50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = x_train.mean(), x_train.std()\n",
    "\n",
    "x_train = normalize(x_train, m, s)\n",
    "x_test = normalize(x_test, m, s)  # normalizing the same way as train part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.1126e-08), tensor(1.))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.mean(), x_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lin1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(784, 128, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.3 ms ± 1.63 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 _=model(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "\n",
    "We will use CE as loss function.\n",
    "\n",
    "\n",
    "First, given logits we need to compute softmax\n",
    "\n",
    "$$ p_i = \\frac{e^{l_i}}{\\sum\\limits_{j=1}^{n} e^{l_j}} $$\n",
    "\n",
    "$$ softmax(l_1, \\ldots, l_n) = (p_1, \\ldots, p_n) $$\n",
    "\n",
    "Cross Entropy Loss - given one hot encoding of the correct class $ y = (0, \\ldots, 1, \\ldots, 0) $, where $1$ in $j$-th position\n",
    "\n",
    "$$ -\\sum\\limits_{i=1}^{n} y_i \\log p_i = - \\log p_j $$\n",
    "\n",
    "Total loss $ - \\frac{1}{l} \\sum\\limits_{i=1}^{l} \\log p_i $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1693, -0.2185, -0.2553,  0.3165, -0.2184, -0.1091,  0.0361, -0.1630,\n",
       "           0.1680,  0.0583]], grad_fn=<AddmmBackward>), tensor(3))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_test[:1]), y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(p, y):\n",
    "    return -p[range(y.shape[0]), y].float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tricks\n",
    "\n",
    "For numerical stability, we don't want to get very hight values of $e^{l_i}$, therefore we can do following\n",
    "\n",
    "$$ p_i = \\frac{e^{l_i - x}}{\\sum_j e^{l_j - x}}, ~\\text{where}~ x = \\max \\{ l_i \\}$$\n",
    "\n",
    "Hence log probability if equal to $ \\log p_i = l_i - x - \\log\\left( \\sum_j e^{l_j - x} \\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 10])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = model(x_test)\n",
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.max(dim=1, keepdim=True).values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(l):\n",
    "    x = l.max(dim=1, keepdim=True).values\n",
    "    l -= x\n",
    "    l -= l.exp().sum(dim=1, keepdim=True).log()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "near(log_softmax(l), F.log_softmax(l, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(l, target):\n",
    "    return nll(log_softmax(l), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "near(cross_entropy_loss(l, y_test), torch.nn.CrossEntropyLoss()(l, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "Lets define accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(l, target):\n",
    "    _, top_indices = l.topk(k=1, dim=1)\n",
    "    acc = (target.unsqueeze(-1) == top_indices).float().mean()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1034)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(l, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "First lets overfit to a single mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3389, grad_fn=<NegBackward>)\n",
      "0.2816978511036816\n"
     ]
    }
   ],
   "source": [
    "model = Model(784, 128, 10)\n",
    "\n",
    "criterion = cross_entropy_loss\n",
    "\n",
    "print(criterion(model(x_test), y_test))\n",
    "\n",
    "lr = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "n_epochs = 1\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(0, x_train.shape[0], batch_size):\n",
    "    \n",
    "        x = x_train[i: i + batch_size]\n",
    "        y = y_train[i: i + batch_size]\n",
    "        \n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.lin1.weight -= model.lin1.weight.grad * lr\n",
    "            model.lin1.bias -= model.lin1.bias.grad * lr\n",
    "            \n",
    "            model.lin2.weight -= model.lin2.weight.grad * lr\n",
    "            model.lin2.bias -= model.lin2.bias.grad * lr\n",
    "    \n",
    "            model.lin1.weight.grad.zero_()\n",
    "            model.lin1.bias.grad.zero_()\n",
    "            \n",
    "            model.lin2.weight.grad.zero_()\n",
    "            model.lin2.bias.grad.zero_()\n",
    "            \n",
    "        acc = accuracy(output, y)\n",
    "        \n",
    "    print(total_loss/(x_train.shape[0] // batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9375)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactoring\n",
    "### Parameters\n",
    "\n",
    "First thing we want to do is to make \n",
    "```Python\n",
    "model.lin1.weight -= model.lin1.weight.grad * lr\n",
    "model.lin1.bias -= model.lin1.bias.grad * lr\n",
    "\n",
    "model.lin2.weight -= model.lin2.weight.grad * lr\n",
    "model.lin2.bias -= model.lin2.bias.grad * lr\n",
    "\n",
    "model.lin1.weight.grad.zero_()\n",
    "model.lin1.bias.grad.zero_()\n",
    "\n",
    "model.lin2.weight.grad.zero_()\n",
    "model.lin2.bias.grad.zero_()\n",
    "```\n",
    "\n",
    "more compact by indroducing `model.parameters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self._modules = {}\n",
    "        \n",
    "        self.lin1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def __setattr__(self, k, v):\n",
    "        if not k.startswith('_'): \n",
    "            self._modules[k] = v\n",
    "        \n",
    "        super().__setattr__(k, v)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(self._modules)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        for layer in self._modules.values():\n",
    "            for p in layer.parameters():\n",
    "                yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(784, 128, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lin1': Linear(in_features=784, out_features=128, bias=True), 'relu': ReLU(), 'lin2': Linear(in_features=128, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2882, grad_fn=<NegBackward>)\n",
      "0.28319782536672417\n"
     ]
    }
   ],
   "source": [
    "model = Model(784, 128, 10)\n",
    "\n",
    "criterion = cross_entropy_loss\n",
    "\n",
    "print(criterion(model(x_test), y_test))\n",
    "\n",
    "lr = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "n_epochs = 1\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(0, x_train.shape[0], batch_size):\n",
    "    \n",
    "        x = x_train[i: i + batch_size]\n",
    "        y = y_train[i: i + batch_size]\n",
    "        \n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= param.grad * lr\n",
    "                param.grad.zero_()\n",
    "            \n",
    "        acc = accuracy(output, y)\n",
    "        \n",
    "    print(total_loss/(x_train.shape[0] // batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Lets replace code \n",
    "\n",
    "```Python\n",
    "with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "        param -= param.grad * lr\n",
    "        param.grad.zero_()\n",
    "```\n",
    "\n",
    "with something more compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    \n",
    "    def __init__(self, parameters, lr=0.1):\n",
    "        self.parameters = list(parameters)\n",
    "        self.lr = lr\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.parameters:\n",
    "                p.grad.zero_()\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.parameters:\n",
    "                p -= p.grad * self.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, criterion, optim, batch_size = 64, n_epochs = 1):\n",
    "    for e in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "\n",
    "        for i in range(0, x_train.shape[0], batch_size):\n",
    "\n",
    "            x = x_train[i: i + batch_size]\n",
    "            y = y_train[i: i + batch_size]\n",
    "\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            acc = accuracy(output, y)\n",
    "            total_acc += acc\n",
    "\n",
    "        print(total_loss/(x_train.shape[0] // batch_size))\n",
    "        print(total_acc/(x_train.shape[0] // batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3469, grad_fn=<NegBackward>)\n",
      "0.28267738204473264\n",
      "tensor(0.9187)\n"
     ]
    }
   ],
   "source": [
    "model = Model(784, 128, 10)\n",
    "\n",
    "criterion = cross_entropy_loss\n",
    "optim = SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "print(criterion(model(x_test), y_test))\n",
    "\n",
    "fit(model, criterion, optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ModuleList and nn.Sequential\n",
    "\n",
    "\n",
    "If in the previous Model class we will replace \n",
    "```Python\n",
    "self.lin1 = nn.Linear(input_dim, hidden_dim)\n",
    "self.relu = nn.ReLU()\n",
    "self.lin2 = nn.Linear(hidden_dim, output_dim)\n",
    "```\n",
    "\n",
    "with \n",
    "\n",
    "```Python\n",
    "self.layers = [nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10)]\n",
    "```\n",
    "\n",
    "it won't work since Python list has no `parameters` attribute because it's not a instance of Module.\n",
    "\n",
    "\n",
    "To solve this problem lets build simple versions of `nn.ModuleList` and `nn.Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Module.add_module?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleList(nn.Module):\n",
    "    \n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        for i, l in enumerate(layers):\n",
    "            self.add_module(f'layer_{i}', l)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self._modules.values())\n",
    "\n",
    "            \n",
    "class Sequential():\n",
    "    \n",
    "    def __init__(self, *layers):\n",
    "        self.layers = ModuleList(layers)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.layers.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3540, grad_fn=<NegBackward>)\n",
      "0.27868394673862773\n",
      "tensor(0.9202)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10))\n",
    "\n",
    "criterion = cross_entropy_loss\n",
    "optim = SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "print(criterion(model(x_test), y_test))\n",
    "\n",
    "fit(model, criterion, optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader\n",
    "\n",
    "First lets try to turn this\n",
    "\n",
    "```Python\n",
    "x = x_train[i: i + batch_size]\n",
    "y = y_train[i: i + batch_size]\n",
    "```\n",
    "\n",
    "into something like this\n",
    "```Python\n",
    "x, y = train_ds[i: i + batch_size]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(x_train, y_train)\n",
    "assert len(train_ds) == x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_ds, model, criterion, optim, batch_size = 64, n_epochs = 1):\n",
    "    for e in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "\n",
    "        for i in range(0, len(train_ds), batch_size):\n",
    "\n",
    "            x, y = train_ds[i: i + batch_size]\n",
    "\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            acc = accuracy(output, y)\n",
    "            total_acc += acc\n",
    "\n",
    "        print(total_loss/(x_train.shape[0] // batch_size))\n",
    "        print(total_acc/(x_train.shape[0] // batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28107381748958526\n",
      "tensor(0.9184)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10))\n",
    "\n",
    "criterion = cross_entropy_loss\n",
    "optim = SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "fit(train_ds, model, criterion, optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's good time to simplify \n",
    "\n",
    "```Python\n",
    "for i in range(0, len(train_ds), batch_size):\n",
    "    x, y = train_ds[i: i + batch_size]\n",
    "```\n",
    "\n",
    "and to do\n",
    "```Python\n",
    "for x, y in train_dataloader:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \n",
    "    def __init__(self, ds, batch_size):\n",
    "        self.ds = ds\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ds) // self.batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.batch_size):\n",
    "            x, y = self.ds[i: i + self.batch_size]\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_dataloader, model, criterion, optim, n_epochs = 1):\n",
    "    for e in range(n_epochs):\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "\n",
    "        for x, y in train_dataloader:\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            acc = accuracy(output, y)\n",
    "            total_acc += acc\n",
    "\n",
    "        print(total_loss / len(train_dataloader))\n",
    "        print(total_acc / len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28311672429917867\n",
      "tensor(0.9192)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(nn.Linear(784, 128), nn.ReLU(), nn.Linear(128, 10))\n",
    "\n",
    "criterion = cross_entropy_loss\n",
    "optim = SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "fit(train_dataloader, model, criterion, optim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
