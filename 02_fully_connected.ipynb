{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.nb_01 import *\n",
    "\n",
    "x, y = get_mnist_data()\n",
    "y = y.unsqueeze(-1)\n",
    "\n",
    "\n",
    "def normalize(x, m, s): return (x - m) / s\n",
    "\n",
    "x_train, x_test = x[:50000], x[50000:]\n",
    "y_train, y_test = y[:50000], y[50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1310), tensor(0.3085))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.mean(), x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, s = x_train.mean(), x_train.std()\n",
    "\n",
    "x_train = normalize(x_train, m, s)\n",
    "x_test = normalize(x_test, m, s)  # normalizing the same way as train part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.1126e-08), tensor(1.))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.mean(), x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer\n",
    "\n",
    "Initialization is the key to successfuly train neural nets\n",
    "\n",
    "* [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)\n",
    "* [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "\n",
    "Basicly speaking, output of each layer idealy should have N(0, 1) distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b):\n",
    "    return x.matmul(w) + b\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "\n",
    "w1 = torch.randn(input_dim, hidden_dim) * math.sqrt(2 / input_dim)\n",
    "b1 = torch.zeros(hidden_dim)\n",
    "\n",
    "w2 = torch.randn(hidden_dim, output_dim) * math.sqrt(2 / hidden_dim)\n",
    "b2 = torch.zeros(output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5765), tensor(0.8422))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = relu(lin(x_test, w1, b1))\n",
    "h.mean(), h.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    x = lin(x, w1, b1)\n",
    "    h = relu(x)\n",
    "    out = lin(h, w2, b2)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.9 ms ± 829 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 _=model(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 1]), torch.Size([10000, 1]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_test).shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_pred, y_true):\n",
    "    return ((y_pred - y_true) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(39.7200)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(model(x_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_grad(inp, y):\n",
    "    inp.g = 2 * (inp - y)\n",
    "\n",
    "\n",
    "def lin_grad(inp, out, w, b):\n",
    "    inp.g = out.g @ w.t()\n",
    "    \n",
    "    # why?\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    b.g = out.g.sum(0)\n",
    "\n",
    "\n",
    "def relu_grad(inp, out):\n",
    "    inp.g = out.g * (inp > 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(x, y, verbose=False):\n",
    "    s1 = lin(x, w1, b1)\n",
    "    l1 = relu(s1)\n",
    "    s2 = lin(l1, w2, b2)\n",
    "    out = mse(s2, y)\n",
    "    \n",
    "    if verbose:\n",
    "        print('x', x.shape, 'y', y.shape)\n",
    "        print('s1', s1.shape)\n",
    "        print('l1', l1.shape)\n",
    "        print('s2', s2.shape)\n",
    "        print('out', out.shape)\n",
    "    \n",
    "    mse_grad(s2, y)\n",
    "    lin_grad(l1, s2, w2, b2)\n",
    "    relu_grad(s1, l1)\n",
    "    lin_grad(x, s1, w1, b1)\n",
    "    \n",
    "    if verbose:\n",
    "        print()\n",
    "        print('s2', s2.g.shape)\n",
    "        print('l1', l1.g.shape)\n",
    "        print('s1', s1.g.shape)\n",
    "        print('x', x.g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([1, 784]) y torch.Size([1, 1])\n",
      "s1 torch.Size([1, 128])\n",
      "l1 torch.Size([1, 128])\n",
      "s2 torch.Size([1, 1])\n",
      "out torch.Size([])\n",
      "\n",
      "s2 torch.Size([1, 1])\n",
      "l1 torch.Size([1, 128])\n",
      "s1 torch.Size([1, 128])\n",
      "x torch.Size([1, 784])\n"
     ]
    }
   ],
   "source": [
    "forward_and_backward(x_test[:1], y_test[:1], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare with PyTorch autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1g = w1.g.clone()\n",
    "b1g = b1.g.clone()\n",
    "w2g = w2.g.clone()\n",
    "b2g = b2.g.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1c = w1.clone().requires_grad_(True)\n",
    "b1c = b1.clone().requires_grad_(True)\n",
    "w2c = w2.clone().requires_grad_(True)\n",
    "b2c = b2.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    s1 = lin(x, w1c, b1c)\n",
    "    l1 = relu(s1)\n",
    "    s2 = lin(l1, w2c, b2c)\n",
    "    return s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(x_test[:1])\n",
    "loss = mse(out, y_test[:1])\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "near(w1g, w1c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules \n",
    "\n",
    "Lets refactor previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.w = torch.randn(in_features, out_features) * math.sqrt(2 / in_features)\n",
    "        self.b = torch.zeros(out_features)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.inp = x\n",
    "        self.out = x.matmul(self.w) + self.b\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        \n",
    "        self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0)\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.inp = x\n",
    "        self.out = x.clamp_min(0.)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g * (self.inp > 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    \n",
    "    def __call__(self, output, target):\n",
    "        self.inp = output\n",
    "        self.target = target\n",
    "        self.out = (output - target).pow(2).mean()\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = 2 * (self.inp - self.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = [\n",
    "            Linear(input_dim, hidden_dim),\n",
    "            Relu(),\n",
    "            Linear(hidden_dim, output_dim)\n",
    "        ]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self):\n",
    "        for l in self.layers[::-1]:\n",
    "            l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "criteria = MSE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(x_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criteria(output, y_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria.backward()\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1g = model.layers[0].w.g.clone()\n",
    "b1g = model.layers[0].b.g.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1c = model.layers[0].w.clone().requires_grad_(True)\n",
    "b1c = model.layers[0].b.clone().requires_grad_(True)\n",
    "\n",
    "w2c = model.layers[2].w.clone().requires_grad_(True)\n",
    "b2c = model.layers[2].b.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    s1 = lin(x, w1c, b1c)\n",
    "    l1 = relu(s1)\n",
    "    s2 = lin(l1, w2c, b2c)\n",
    "    return s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(x_test[:1])\n",
    "loss = mse(out, y_test[:1])\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "near(w1g, w1c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continue refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        self.inps = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        return self._backward(self.out, *self.inps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.w = torch.randn(in_features, out_features) * math.sqrt(2 / in_features)\n",
    "        self.b = torch.zeros(out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x @ self.w + self.b\n",
    "    \n",
    "    \n",
    "    def _backward(self, out, inp):\n",
    "        inp.g = out.g @ self.w.t()\n",
    "        \n",
    "        self.w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "        self.b.g = out.g.sum(0)\n",
    "    \n",
    "    \n",
    "class Relu(Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.clamp_min(0.)\n",
    "    \n",
    "    def _backward(self, out, inp):\n",
    "        inp.g = out.g * (inp > 0).float()\n",
    "    \n",
    "    \n",
    "class MSE(Module):\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        return (output-target).pow(2).mean()\n",
    "    \n",
    "    def _backward(self, out, inp, target):\n",
    "        inp.g = 2 * (inp - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = [\n",
    "            Linear(input_dim, hidden_dim),\n",
    "            Relu(),\n",
    "            Linear(hidden_dim, output_dim)\n",
    "        ]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self):\n",
    "        for l in reversed(self.layers):\n",
    "            l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "criteria = MSE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(x_test[:1])\n",
    "loss = criteria(output, y_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria.backward()\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1g = model.layers[0].w.g.clone()\n",
    "b1g = model.layers[0].b.g.clone()\n",
    "\n",
    "w1c = model.layers[0].w.clone().requires_grad_(True)\n",
    "b1c = model.layers[0].b.clone().requires_grad_(True)\n",
    "\n",
    "w2c = model.layers[2].w.clone().requires_grad_(True)\n",
    "b2c = model.layers[2].b.clone().requires_grad_(True)\n",
    "\n",
    "def model(x):\n",
    "    s1 = lin(x, w1c, b1c)\n",
    "    l1 = relu(s1)\n",
    "    s2 = lin(l1, w2c, b2c)\n",
    "    return s2\n",
    "\n",
    "out = model(x_test[:1])\n",
    "loss = mse(out, y_test[:1])\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "near(w1g, w1c.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
